# Bit√°cora t√©cnica: Clasificaci√≥n multimodal de emociones en m√∫sica 
**Art√≠culo analizado:** *Music Emotion Classification Method Based on Deep Learning and Explicit Sparse Attention Network*  
**Proyecto MIR:** *An√°lisis de emoci√≥n y sentimiento en m√∫sica desde una perspectiva multimodal*

--- 
## 1. Prop√≥sito del art√≠culo
El art√≠culo propone un m√©todo para mejorar la clasificaci√≥n de emociones en m√∫sica mediante una arquitectura de red profunda que combina t√©cnicas de procesamiento ac√∫stico y atenci√≥n selectiva. Su objetivo principal es abordar los desaf√≠os de ruido, subjetividad y alta dimensionalidad presentes en datos musicales reales, proponiendo un modelo que:

- Integra espectrogramas y descriptores ac√∫sticos cl√°sicos (LLDs/HSFs).
- Utiliza una arquitectura h√≠brida CNN-LSTM para capturar patrones espaciales y temporales.
- Introduce un mecanismo de atenci√≥n expl√≠citamente dispersa (Explicit Sparse Attention Network) que filtra activamente las caracter√≠sticas irrelevantes.
- Clasifica canciones en cuatro emociones b√°sicas: **happy**, **sad**, **relax** y **anger**.

El enfoque se centra exclusivamente en se√±ales ac√∫sticas, sin incorporar informaci√≥n l√≠rica o textual, pero con un pipeline robusto y replicable.


## 2. Arquitectura propuesta

El modelo propuesto combina tres componentes principales:

### üîπ 1. Modelo h√≠brido CNN-LSTM + DNN
- **CNN**: extrae caracter√≠sticas espaciales de espectrogramas mediante tres capas de convoluci√≥n + max pooling.
- **LSTM bidireccional**: captura dependencias temporales en la secuencia de espectrogramas procesados.
- **DNN**: procesa los descriptores ac√∫sticos cl√°sicos (LLDs), convertidos en HSFs mediante estad√≠sticas (media, varianza, m√°ximo).
- Las salidas de CNN-LSTM y DNN se concatenan y se env√≠an a una capa Softmax para clasificaci√≥n.

### üî∏ 2. Fusi√≥n de espectrogramas + LLDs/HSFs
- **Espectrogramas**: tratados como im√°genes RGB de tama√±o 512√ó512√ó4.
- **LLDs**: incluyen MFCC-13, ZCR, centroides espectrales, bandwidth, flux, roll-off, cromaticidad.
- **HSFs**: se obtienen aplicando estad√≠sticas sobre los LLDs.
- Esta fusi√≥n permite representar tanto la estructura espectral como los patrones ac√∫sticos globales.

### üî∫ 3. Mecanismo de atenci√≥n expl√≠citamente dispersa
Este componente reemplaza la atenci√≥n est√°ndar por una variante que **filtra activamente las caracter√≠sticas irrelevantes** antes del softmax.

#### üß© Funcionamiento paso a paso:
1. Se calcula la matriz de atenci√≥n $K$ como en el mecanismo est√°ndar (producto escalar entre claves y consultas).
2. Para cada fila $u$ de $K$, se identifica el umbral $t_u$ correspondiente al *top-d* valores m√°s altos.
3. Se aplica una **m√°scara** que conserva solo los elementos $k_{um} \geq t_u$, y asigna $-\infty$ al resto:
$$ M(K, D ) = \begin{cases} k_{um}, & \text{si } k_{um} \geq t_u \\ -\infty, & \text{si } k_{um} < t_u \end{cases} $$
- Esto fuerza a que el softmax posterior asigne peso cero a los elementos descartados.
4. Se normaliza la matriz enmascarada con softmax, obteniendo una distribuci√≥n de atenci√≥n **concentrada en los elementos m√°s relevantes**.
5. Durante backpropagation, se calcula el gradiente solo sobre los elementos seleccionados, lo que reduce la dispersi√≥n y mejora la eficiencia.

#### ‚öôÔ∏è Implementaci√≥n pr√°ctica:
- Puede implementarse como una capa personalizada en PyTorch o TensorFlow.
- Requiere definir:
  - Umbral din√°mico por fila (top-d).
  - Operaci√≥n de m√°scara antes del softmax.
  - Propagaci√≥n de gradientes solo sobre elementos seleccionados.
- Ideal para tareas con ruido o alta dimensionalidad, como m√∫sica real con mezcla vocal/instrumental.

### üéØ Clasificaci√≥n final
- La salida fusionada se clasifica en una de cuatro emociones: `happy`, `sad`, `relax`, `anger`.
- Se utiliza **votaci√≥n por bloques segmentados** para decidir la emoci√≥n dominante de cada canci√≥n, es decir, se divide la canci√≥n en segmentos (10s, 15s, 25s), se clasifica cada segmento y se elige la emoci√≥n m√°s frecuente.


## 3. Resultados clave

El art√≠culo presenta una serie de experimentos comparativos que demuestran la efectividad del enfoque propuesto. Los resultados m√°s relevantes son:

### üìä Precisi√≥n de clasificaci√≥n
- **Accuracy promedio del modelo completo**: `0.712`
- Emociones individuales:
  - Happy: `0.737`
  - Sad: `0.723`
  - Relax: `0.698`
  - Anger: `0.688`

### üîç Comparaci√≥n de mecanismos de atenci√≥n
| Mecanismo de atenci√≥n         | Accuracy | Cross-entropy |
|------------------------------|----------|----------------|
| Tradicional (soft attention) | 0.682    | 0.654          |
| Expl√≠citamente dispersa      | **0.712**| **0.631**      |

La atenci√≥n dispersa mejora tanto la precisi√≥n como la entrop√≠a cruzada, indicando una distribuci√≥n m√°s enfocada y eficiente.

### üß™ Comparaci√≥n de preprocesamiento
| M√©todo de preprocesamiento             | Accuracy promedio |
|----------------------------------------|-------------------|
| Segmentaci√≥n fina                      | 0.650             |
| Separaci√≥n vocal                       | 0.653             |
| Segmentaci√≥n + separaci√≥n              | 0.679             |
| M√©todo completo (con atenci√≥n dispersa)| **0.712**         |

Separar la voz del fondo musical y aplicar segmentaci√≥n mejora significativamente el rendimiento del modelo, validando la utilidad de estas t√©cnicas en entornos ac√∫sticos complejos.



## 4. Dataset utilizado

El art√≠culo no utiliza un dataset p√∫blico ni proporciona enlaces a recursos externos. En su lugar, propone construir un corpus propio a partir de plataformas musicales chinas, siguiendo estos criterios:

- Selecci√≥n de canciones con m√°s de 3 millones de reproducciones, para asegurar relevancia y popularidad.
- Uso de listas de reproducci√≥n etiquetadas emocionalmente (*happy*, *sad*, *relax*, *anger*).
- Filtrado por calidad de audio, duraci√≥n y lenguaje.
- Resultado final: 2147 canciones divididas en conjunto de entrenamiento (80%) y prueba (20%).

### üîç Observaciones
- El procedimiento es **general y flexible**, lo que permite replicarlo con otras plataformas (Spotify, YouTube Music, etc.).
- No se incluyen metadatos, letras ni anotaciones manuales.
- Las etiquetas emocionales provienen de las playlists, lo que implica una **etiquetaci√≥n d√©bil pero escalable**.

Este enfoque es √∫til como gu√≠a para construir datasets personalizados, aunque no garantiza comparabilidad directa con otros estudios.


## 5. C√≥mo recrear el dataset
## üõ†Ô∏è C√≥mo recrear el dataset

Pasos sugeridos para construir un corpus emocional ac√∫stico replicable:

1. **Selecci√≥n de playlists con etiquetas emocionales**
   - Buscar listas de reproducci√≥n p√∫blicas con etiquetas como `happy`, `sad`, `relax`, `anger`.
   - Priorizar aquellas con alto n√∫mero de reproducciones para asegurar coherencia emocional.

2. **Filtrado por calidad, duraci√≥n y lenguaje**
   - Eliminar canciones con baja calidad de audio, duraci√≥n at√≠pica (<1 min o >6 min), o idiomas no deseados.
   - Opcional: filtrar por g√©nero musical si se desea controlar la variabilidad.

3. **Descarga de audio (MP3)**
   - Usar herramientas de extracci√≥n desde plataformas musicales (respetando t√©rminos de uso).
   - Guardar los archivos con metadatos b√°sicos: t√≠tulo, artista, etiqueta emocional.

4. **Segmentaci√≥n en bloques (10s, 15s, 25s)**
   - Dividir cada canci√≥n en fragmentos temporales fijos.
   - Cada bloque se trata como una unidad de an√°lisis independiente.
   - Esta estrategia simula la percepci√≥n humana: juzgamos la emoci√≥n de una canci√≥n por c√≥mo se desarrolla, no por un instante aislado.
   - Permite aplicar clasificaci√≥n por fragmento y luego realizar **votaci√≥n por mayor√≠a** para determinar la emoci√≥n dominante.

5. **Separaci√≥n de voz y fondo (Spleeter/Demucs)**
   - Aplicar separaci√≥n de fuentes para obtener:
     - Pista vocal pura
     - Pista instrumental pura
   - Esto permite analizar por separado las emociones transmitidas por letra y m√∫sica.

6. **Extracci√≥n de caracter√≠sticas ac√∫sticas**
   - Por bloque y por pista (voz/fondo), extraer:
     - MFCC
     - ZCR
     - Spectral Centroid, Bandwidth, Flux, Roll-off
     - Chroma features
   - Usar herramientas como Librosa o Essentia.

7. **Etiquetado por playlist o curaci√≥n manual**
   - Asignar la etiqueta emocional de la playlist a cada canci√≥n.
   - Opcional: realizar curaci√≥n manual para verificar coherencia emocional o ajustar clases.

## 6. Flujo de trabajo replicable
- Preprocesamiento: segmentaci√≥n + separaci√≥n de fuentes
- Extracci√≥n de caracter√≠sticas: espectrogramas + LLDs
- Modelado: CNN-LSTM + DNN + atenci√≥n dispersa
- Inferencia: clasificaci√≥n por bloque + votaci√≥n por mayor√≠a

## 7. Ideas aprovechables para proyecto MIR
Este art√≠culo ofrece varias estrategias t√©cnicas que pueden integrarse o adaptarse al proyecto de an√°lisis emocional multimodal en m√∫sica:

### üî∫ Mecanismo de atenci√≥n expl√≠citamente dispersa
- Reemplaza la atenci√≥n est√°ndar por una variante que filtra activamente las caracter√≠sticas irrelevantes.
- Mejora la precisi√≥n y la eficiencia del modelo en entornos ruidosos o con alta dimensionalidad.
- Ideal para m√∫sica real con mezcla vocal/instrumental.

### üîÑ Votaci√≥n por bloques segmentados
- Divide cada canci√≥n en fragmentos temporales (10s, 15s, 25s).
- Clasifica cada bloque por separado y aplica votaci√≥n por mayor√≠a para determinar la emoci√≥n dominante.
- Simula la percepci√≥n humana acumulativa de la emoci√≥n musical.

### üéº Separaci√≥n de fuentes (voz/fondo)
- Permite analizar por separado las emociones transmitidas por la letra y por la instrumentaci√≥n.
- Mejora la concentraci√≥n de las caracter√≠sticas ac√∫sticas y la precisi√≥n del modelo.

### üîÄ Fusi√≥n de representaciones ac√∫sticas
- Combina espectrogramas (procesados por CNN-LSTM) con descriptores ac√∫sticos cl√°sicos (LLDs/HSFs procesados por DNN).
- Aporta una visi√≥n multimodal dentro del dominio ac√∫stico, sin necesidad de texto.

### üß™ Flujo replicable con herramientas abiertas
- Todo el pipeline puede implementarse con herramientas como Librosa, Spleeter/Demucs, TensorFlow/Keras o PyTorch.
- La arquitectura es modular y adaptable a datasets personalizados.



## 8. Limitaciones
- No incluye an√°lisis l√≠rico ni embeddings textuales
- Dataset limitado a 4 emociones, no reportan recursos p√∫blicos disponibles ni un procedimiento detallado de recolecci√≥n m√°s all√° de la plataforma china y etiquetas de playlists, esto dificulta la replicabilidad exacta, m√°s all√° de la gu√≠a general proporcionada.
- No se reporta validaci√≥n cruzada ni an√°lisis de overfitting

