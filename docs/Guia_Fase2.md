# Gu√≠a de Transici√≥n: Fase 2 (Extracci√≥n Distribuida y Procesamiento)

**Fecha:** Noviembre 2025  
**Contexto:** Migraci√≥n del Pipeline FMA (Legacy) al Pipeline V2 (Spotify/Genius/YouTube).

---

## 1. ¬øPor qu√© cambiamos todo?

El dataset original (FMA) presentaba un problema cr√≠tico: la intersecci√≥n entre "Audio Disponible" + "Idioma Ingl√©s" + "Etiquetas Emocionales" era demasiado peque√±a (< 2,000 muestras) y de hecho se volvi√≥ cero en cuanto se intentaron recuperar sus letras con Genius.

**La Nueva Estrategia (Pipeline V2):**
En lugar de filtrar lo que hay (buscando datos que cumplieran con todas las condiciones), hemos construido nuestro propio dataset desde cero:
1.  **Semillas:** 30,000+ canciones extra√≠das de Kaggle (Spotify Tracks), balanceadas por cuadrantes emocionales (Happy, Sad, Angry, Relaxed).
2.  **Letras:** Scrapeadas via Genius API (filtrando instrumentales e idioma no-ingl√©s).
3.  **Audio:** Descarga directa desde YouTube.

**Estado Actual:**
Tenemos un archivo maestro limpio (`metadata_step2_lyrics_clean.csv`) con **~6,500 canciones** perfectamente etiquetadas y con letra. Ahora falta terminar de descargar los audios.

---

## 2. Configuraci√≥n del Entorno (CR√çTICO ‚ö†Ô∏è)

Para que el nuevo pipeline funcione, las dependencias han cambiado significativamente (Python 3.11, soporte nativo de tipos, librer√≠as de audio modernas).

**Acci√≥n Requerida:**
Por favor, elimina tu entorno anterior y cr√©alo de nuevo usando el archivo `environment.yaml` actualizado en el repositorio.

```bash
# 1. Desactivar y eliminar el viejo
conda deactivate
conda remove --name mem-env --all

# 2. Crear el nuevo (Asegura tener el environment.yaml actualizado)
conda env create -f environment.yaml

# 3. Activar
conda activate mem-env
```
## 3. Estrategia de descarga distribuida ("Divide y Vencer√°s")

YouTube bloquea temporalmente las IPs que descargan demasiado r√°pido. Para evitar esto y acelerar el proceso, dividiremos la carga de trabajo en dos mitades paralelas.

**Paso A: Generar los archivos divididos**

Uzi ejecutar√° el script de divisi√≥n que genera dos archivos en `data/raw_v2/`:

- `metadata_part_uzi.csv` (Primera mitad)

- `metadata_part_brenda.csv` (Segunda mitad)

**Paso B: Tu Misi√≥n de Descarga**

1. Abre el archivo src/ExtractDataV2/main.py.

2. Busca la l√≠nea donde se define `CSV_STEP2` dentro de `src/ExtractDataV2/main.py` y modif√≠cala para apuntar a tu parte:

    ```bash
    # --- EN main.py ---
    # CSV_STEP2 = RAW_V2_DIR / "metadata_step2_lyrics_clean.csv"  <-- COMENTAR ESTA
    CSV_STEP2 = RAW_V2_DIR / "metadata_part_brenda.csv"           # <-- USAR ESTA
    ```
3. Ejecuta el pipeline de descarga:

    ```bash
    python src/ExtractDataV2/main.py
    ```
4. El script saltar√° autom√°ticamente los Pasos 1 y 2, e iniciar√° la descarga de audios en el Paso 3.

**Nota**: Si YouTube te bloquea (Error "Sign in to confirm..."), det√©n el script, cambia tu IP (reinicia m√≥dem o usa datos) y vuelve a intentar. **El script es incremental, no perder√°s progreso**.

## 4. Arquitectura de la Fase 2: Procesamiento de los Datos Multimodales

Una vez tengamos descargados todos los audios, entraremos a la fase de **Ingenier√≠a de Caracter√≠sticas**. Hemos dise√±ado una estructura modular para trabajar en paralelo sin conflictos.

Para que el modelo multimodal funcione correctamente, necesitamos transformar los datos crudos (MP3 y Texto Raw) en **tensores** y **vectores** organizados. No crearemos un unico archivo gigante que contenga todo, sino una estructura modular de archivos todos vinculados por el `spotify_id`.


El modelo consumir√° 4 flujos de datos. Proponemos esta estructura de archivos finales en `data/processed/`:
 
1. `master_dataset.csv`: El cerebro. Contiene `spotify_id`, etiquetas ($Y$: valence, arousal, quadrant), metadatos (artista, t√≠tulo) y rutas a los archivos pesados.
2. `features_1d/` (Carpeta): Aqu√≠ vivir√°n los datos 1D ligeros en formato CSV.
   - `features_audio_1d.csv`: Tabla con los HSFs (media, varianza de MFCCs, etc.) para cada ID.
   - `features_text_1d.csv`: Tabla con vectores TF-IDF o estad√≠sticos para cada ID.
4. `features_2d/` (Carpeta): Aqu√≠ vivir√°n los datos 2D pesados en formato .npy (NumPy binario).
   - `features_2d/spectrograms/{id}.npy`: Matriz del espectrograma (Audio 2D).
   - `features_2d/embeddings/{id}.npy`: Matriz de embeddings BERT/Word2Vec (Texto 2D).

### 4.1. El Roadmap Paso a Paso de la Fase de Procesamiento

El proceso se divide en 3 etapas l√≥gicas para llegar a esta arquitectura:

---
**Etapa 1: La Gran Alineaci√≥n (The Great Alignment)**
* **Script:** `src/ProcessData/utils/alignment.py` (Ya implementado).
* **Acci√≥n:** Escanea la carpeta f√≠sica `audio/` y la cruza con el CSV limpio de letras `metadata_step2_lyrics_clean.csv`. Es decir, valida cu√°les letras de las 6,500 tienen audio descargado.
* **Output Cr√≠tico:** `data/interim/aligned_metadata.csv`.

    > - **NOTA PARA BRENDA:** Este archivo es tu **"Lista de Tareas"**. Tus scripts deben leer este CSV para saber qu√© canciones procesar. No uses los archivos de `raw_v2`.
---
**Etapa 2: Extracci√≥n de Caracter√≠sticas (Paralelo)**
Aqu√≠ nos dividimos el trabajo. Ambos leemos la "Lista de Tareas" y generamos archivos en `data/processed/`.

* **Rama Audio (Uzi):** Genera HSFs (**1D**) y Espectrogramas (**2D**).

    > **Nota**: Uzi se acaba de dar cuenta que los HSFs son los que realmente importan para el modelo y no directamente los LLDs, por si acaso tambien compartias esa duda.
* **Rama Texto (Brenda):** Genera TF-IDF/Chi2 (**1D**) y Embeddings BERT 
(**2D**).

    > **Nota**: Puedes evaluar si usar TF-IDF o Chi2 para la representaci√≥n 1D o incluso una representaci√≥n m√°s reciente pero ligera. Lo importante es que las representaciones sean generadas en un formato adecuado.
---
**Etapa 3: El Archivo Maestro**

Una vez que ambos hayamos generado nuestras caracter√≠sticas, se ejecutar√° un script final que cruce todos los outputs y genere el archivo maestro final. 
Este script final hace Merge de `aligned_metadata.csv` con `features_1d` y `features_2d` usando `spotify_id`.
* **Output:** El archivo generado estar√° ubicado en `data/processed/master_dataset.csv`.
* **Funci√≥n:** Es el √≠ndice final que validar√° que, para cada fila (`spotify_id`), existan tanto los archivos de audio como los de texto y tengan correctamente asignados los vectores correspondientes asi como las etiquetas emocionales. Por lo tanto, el modelo leer√° este archivo para conectarse a los datos cuando entrene.

### 4.2. Estructura de la fase 2

Para la creacion de los modulos del procesamiento se dise√±√≥ la siguiente **Estructura de Carpetas en ProcessData:**

```bash
src/
‚îî‚îÄ‚îÄ ProcessData/             <-- NUEVA CARPETA FASE 2
    ‚îú‚îÄ‚îÄ main_processing.py   <-- Orquestador
    ‚îÇ
    ‚îú‚îÄ‚îÄ utils/   # Funciones compartidas (Carga de archivos, Alineaci√≥n, etc.)
    ‚îÇ   ‚îú‚îÄ‚îÄ alignment.py     <-- Paso 1 (Cruza CSV vs Carpeta Audio)
    ‚îÇ   ‚îî‚îÄ‚îÄ io_utils.py      <-- Funciones para guardar/cargar .npy
    ‚îÇ
    ‚îú‚îÄ‚îÄ audio/      # (Responsable: Uzi) - Recorte, Espectrogramas, HSFs
    ‚îÇ   ‚îú‚îÄ‚îÄ trimming.py      <-- L√≥gica de corte (30s -> 15s por Energ√≠a)
    ‚îÇ   ‚îú‚îÄ‚îÄ features_1d.py   <-- Librosa -> HSFs
    ‚îÇ   ‚îî‚îÄ‚îÄ spectrograms.py  <-- Librosa -> MelSpec -> .npy
    ‚îÇ
    ‚îî‚îÄ‚îÄ text/  # (Responsable: Brenda) - Limpieza, TF-IDF, Embeddings
        ‚îú‚îÄ‚îÄ cleaning.py      <-- Regex y NLTK/Spacy
        ‚îú‚îÄ‚îÄ features_1d.py   <-- TF-IDF (Scikit-learn)
        ‚îî‚îÄ‚îÄ embeddings.py    <-- Transformers/Gensim -> .npy
```

Una vez generados, los scripts deben depositar los resultados siguiendo esta estructura sim√©trica para que la integraci√≥n sea autom√°tica y limpia:

```bash
data/processed/
‚îÇ
‚îú‚îÄ‚îÄ features_1d/              # Tablas num√©ricas (CSV)
‚îÇ   ‚îú‚îÄ‚îÄ features_audio_1d.csv    # (Uzi) Estad√≠sticas de audio (HSFs)
‚îÇ   ‚îî‚îÄ‚îÄ features_text_1d.csv   # (Brenda) Vectores TF-IDF/Chi2
‚îÇ
‚îú‚îÄ‚îÄ features_2d/              # Tensores pesados (NumPy Binary)
‚îÇ   ‚îú‚îÄ‚îÄ spectrograms/         # (Uzi) Matrices .npy de Audio
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ 0AcJ0e....npy
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îÇ   ‚îî‚îÄ‚îÄ embeddings/           # (Brenda) Matrices .npy de Texto
‚îÇ       ‚îú‚îÄ‚îÄ 0AcJ0e....npy
‚îÇ       ‚îî‚îÄ‚îÄ ...
‚îÇ
‚îî‚îÄ‚îÄ master_dataset.csv        # (Final) √çndice validado
```
### 4.3 Especificaciones para el desarrollo de la Rama de Audio

Este modulo se encargar√° de la se√±al acustica.

#### A (Recorte Inteligente) `trimming.py`: 
Cargar los 30s descargados. Calcular la energ√≠a RMS en ventanas deslizantes para encontrar los 15 segundos de mayor intensidad (probablemente el coro) y descartar el resto, de esta forma intentamos capturar la parte **emocionalmente m√°s representativa** de la canci√≥n.

#### B (Extracci√≥n 1D - HSFs) `features_1d.py`: 
Calcular MFCCs, Chroma, ZCR y sacar sus estad√≠sticas (media, std). Guardar en features_audio_1d.csv.

#### C (Extracci√≥n 2D - Espectrogramas) `spectrograms.py`: 
Generar el Mel-Spectrogram de los 15s recortados.

>**Decisi√≥n T√©cnica**: No debemos guardar los espectrogramas como im√°genes (.png), sino como matrices num√©ricas (.npy). Esto evita p√©rdidas de compresi√≥n y facilita la carga en PyTorch/TensorFlow.


### 4.4. Especificaciones para el desarrollo de la Rama de Texto

Mientras Uzi se encarga de procesar los audios (cuando finalmente se descarguen todos), Brenda estar√° a cargo de la inteligencia del Texto. Es necesario que se desarrollen los siguientes m√≥dulos dentro de `src/ProcessData/text/`, siguiendo las mejores pr√°cticas descritas m√°s adelante.

#### A. Limpieza `cleaning.py`:

- Funci√≥n que reciba el string raw de Genius.

- Elimine etiquetas como `[Chorus]`, `[Verse 1]` y arregle errores de codificaci√≥n (muy importante).

- Aplique preprocesamiento cl√°sico: elimine caracteres especiales y normalice (lowercase), lemmatice si es necesario, remueva stopwords.

- Devuelva el texto limpio listo para vectorizaci√≥n. Considera formatos adecuados para esto como `json`. Considero que para capturar bien la estrcutrua de una letra, es mejor no eliminar los saltos de l√≠nea, sino todo lo contrario, preservarlos es importante para que el modelo entienda la estructura de la canci√≥n (verso, coro, puente, etc), estos saltos de linea pueden ser representados en el texto limpio como `\n` y almenos los embeddings generados por `BERT` entienden muy bien esta representaci√≥n.

> **Nota**: El preprocesamiento es importante en t√©cnicas como *TF-IDF*, pero para `word embeddings` debe ser ligero para no perder contexto emocional y sem√°ntico. Considera que `BERT` ya maneja mucho de esto internamente por lo que para los `word embeddings` no es necesario un preprocesamiento agresivo, incluso puede ser contraproducente.

#### B. Features 1D `features_1d.py`:

- **Objetivo**: Generar representaciones estad√≠sticas ligeras.

- **M√©todo**: *TF-IDF*, *Chi2* u otra t√©cnica ligera para convertir el texto limpio en un vector num√©rico fijo por canci√≥n.

- **Salida**: Un archivo `features_text_tfidf.csv` en la carpeta `features_1d`.

    - *Ruta*: `data/processed/features_1d/features_text_tfidf.csv`.

    - *Formato*: CSV o algun otro formato adecuado para este tipo de representaciones.

    - *Columnas*: `spotify_id` (Obligatorio) + columnas del vector.

> **Nota importante:**
El formato CSV es ideal por compatibilidad, pero ineficiente para matrices gigantes.
Como estamos considerando guardar estos vectores en CSV, **es vital limitar la dimensionalidad** para no generar archivos gigantes llenos de ceros (por su sparsity).
>* Configura tu `TfidfVectorizer` con `max_features=1000` (o usa `SelectKBest` con Chi2 para seleccionar los top 1000).
>* *Raz√≥n:* Un vector de 1,000 dimensiones generalmente es adecuado para mezclar con el audio. Un vector de 20,000 (vocabulario completo) ser√≠a inmanejable en formato CSV. Verifica el tama√±o adecuado para que el archivo final no sea demasiado grande pero que a√∫n capture suficiente informaci√≥n.


#### C. Features 2D `embeddings.py`:

Esta es la pieza clave. Necesitamos una funci√≥n que cargue un modelo Transformer (`BERT`) y convierta el texto limpio en un tensor/vector. Tip: Dise√±a la funci√≥n para que reciba el texto y devuelva un `numpy array` (`.npy`).

- **Objetivo**: Generar representaciones sem√°nticas profundas.

- **M√©todo**: Usar un modelo Transformer (ej. `DistilBERT` o `BERT`) para convertir el texto limpio en un tensor, es decir, una matriz de dimensiones (`tokens` x `embedding_size`), donde cada fila representa el embedding de un token y cada columna una dimensi√≥n del embedding.

- **Salida**: Un archivo `.npy` individual por cada canci√≥n.

    - *Ruta*: `data/processed/features_2d/embeddings/{spotify_id}.npy`.

    - *Formato*: Array de NumPy.

> **Nota T√©cnica**: Aseg√∫rate de que tus scripts de extracci√≥n guarden el `spotify_id` para poder hacer el cruce al final.



## 5. Filosof√≠a de C√≥digo para la Fase 2 (Best Practices) üí°
Dado que vamos a integrar tu c√≥digo de texto con el pipeline de audio para correrlo masivamente en una GPU, necesitamos seguir ciertas pautas de ingenier√≠a de software para que todo encaje como piezas de LEGO.

1. **Adi√≥s a los Notebooks (.ipynb) en Producci√≥n**

- Los notebooks son geniales para explorar, pero para el pipeline final necesitamos archivos `.py` pues los archivos `.py` se pueden importar entre s√≠. Un notebook no.

- **Flujo de trabajo**: Prototipa en Colab/Jupyter Notebooks si quieres, pero el c√≥digo final debe estar limpio en `src/ProcessData/text/tus_scripts.py`.

2. **Funciones Puras (Modularidad)**

- Evita escribir c√≥digo que se ejecute "suelto" al inicio del archivo. Todo debe estar dentro de funciones.
- De esta forma, el pipeline puede llamar a tus funciones cuando lo necesite. Por ejemplo, Uzi puede importar `from text.cleaning import limpiar_texto` y aplicarlo a las 6,000 canciones autom√°ticamente.

3. **Rutas Relativas (Pathlib) y Configuraci√≥n Centralizada**

- Evita usar rutas absolutas como `C:/Users/Brenda/....` Eso romper√° el c√≥digo en la otra computadora.

- Usa siempre `pathlib` basado en la ra√≠z del proyecto (ya configurado en `main.py`).

4. **Preparaci√≥n para GPU (C√≥digo y Dependencias)**

- Parametrizaci√≥n del Device: Cuando dise√±es las funciones para BERT/Embeddings, evita "hardcodear" el uso de CPU. Estructura tu funci√≥n para aceptar un par√°metro device:

```Python
def get_embedding(text, model, device='cpu'):
    # El orquestador le pasar√° 'cuda' cuando Uzi corra el script en la GPU
    # Ejemplo interno: inputs = tokenizer(text, ...).to(device)
    pass
```
- **Verificaci√≥n de Dependencias (CUDA)**: El archivo `environment.yaml` actual instala transformers, lo cual usualmente instala torch (PyTorch) como dependencia. Sin embargo, a veces los gestores de paquetes descargan la versi√≥n **CPU-only** por defecto para ahorrar espacio.

- **Tu Misi√≥n**: Investiga y verifica si necesitamos especificar una versi√≥n de PyTorch compatible con CUDA (ej. pytorch-cuda en Conda).

- **Acci√≥n**: Si encuentras que se necesita una instalaci√≥n espec√≠fica para habilitar la GPU, por favor actualiza el `environment.yaml` o agrega una nota en el c√≥digo, no solo con esa dependencia en especifico, sino con cualquier otra que sea necesaria. Esto es vital para que Uzi pueda correr el pipeline completo en GPU sin problemas con el entorno adecuado.

## 6. Eso es todo por ahora!