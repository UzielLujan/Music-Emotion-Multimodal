# Bit√°cora t√©cnica: Clasificaci√≥n multimodal de emociones en m√∫sica  
**Art√≠culo analizado:** *A Multimodal Music Emotion Classification Method Based on Multifeature Combined Network Classifier*  
**Proyecto MIR:** *An√°lisis de emoci√≥n y sentimiento en m√∫sica desde una perspectiva multimodal*

---

## 0. üßæ Resumen sintetizado del art√≠culo

El art√≠culo propone un sistema de clasificaci√≥n de emociones musicales basado en una arquitectura multimodal que combina caracter√≠sticas ac√∫sticas y textuales. Utiliza una red CNN-LSTM para procesar entradas 2D (espectrogramas y Word2Vec) y una red DNN para entradas 1D (LLDs y vectores CHI). La fusi√≥n se realiza mediante *stacking ensemble learning*, evitando p√©rdida de informaci√≥n por reducci√≥n dimensional. Se aplican t√©cnicas de preprocesamiento como separaci√≥n de voz humana y segmentaci√≥n fina de audio. El modelo alcanza una precisi√≥n promedio del 78% en clasificaci√≥n multimodal, superando m√©todos tradicionales de fusi√≥n por caracter√≠sticas o decisiones.

---

## 1. üéØ Motivaci√≥n y objetivos del estudio

- **¬øQu√© problema aborda el art√≠culo?**  
  La clasificaci√≥n de emociones musicales ha sido abordada tradicionalmente desde modelos unimodales, centrados √∫nicamente en caracter√≠sticas ac√∫sticas o textuales. Estos enfoques presentan limitaciones al tratar con la heterogeneidad de las fuentes de informaci√≥n (audio vs letras), lo que afecta la precisi√≥n y riqueza de la clasificaci√≥n emocional.

- **¬øPor qu√© es relevante en MIR?**  
  La recuperaci√≥n de informaci√≥n musical (MIR) se beneficia directamente de una clasificaci√≥n emocional precisa, ya que permite organizar, recomendar y analizar m√∫sica en funci√≥n de estados afectivos. Incorporar m√∫ltiples modalidades mejora la comprensi√≥n del contenido musical y su impacto emocional, aline√°ndose con tendencias actuales en inteligencia artificial aplicada a m√∫sica.

- **¬øQu√© propone como soluci√≥n?**  
  El art√≠culo propone un sistema de clasificaci√≥n multimodal que combina:
  - Una arquitectura CNN-LSTM para procesar caracter√≠sticas 2D (espectrogramas y Word2Vec).
  - Una red DNN para caracter√≠sticas 1D (LLDs y vectores CHI).
  - Un m√©todo de fusi√≥n por *stacking ensemble learning*, que evita la p√©rdida de informaci√≥n por reducci√≥n dimensional y mejora la integraci√≥n de modalidades heterog√©neas.
  Adem√°s, se aplica un preprocesamiento especializado del audio (segmentaci√≥n fina y separaci√≥n de voz) para optimizar la calidad de las caracter√≠sticas extra√≠das.

---

## 2. üß† Arquitectura del modelo propuesto (Multifeature Combined Network Classifier)


### 2.1 Modelo CNN-LSTM previo

El art√≠culo parte de modelos anteriores que combinan redes convolucionales (CNN) y redes de memoria a largo plazo (LSTM) para tareas de clasificaci√≥n de audio o texto por separado. Estos modelos han demostrado buen desempe√±o en:

- **Clasificaci√≥n de emociones en voz**: usando espectrogramas como entrada para CNN-LSTM.
- **Clasificaci√≥n de texto largo**: aplicando CNN para compresi√≥n de caracter√≠sticas y LSTM para secuencias.

Sin embargo, presentan limitaciones cuando se aplican directamente a m√∫sica:
- El audio musical es m√°s largo y complejo que el habla.
- Las letras tienen alta dimensionalidad y dispersi√≥n sem√°ntica.
- Solo utilizan caracter√≠sticas 2D (espectrogramas o Word2Vec), ignorando otras representaciones √∫tiles como LLDs o vectores de frecuencia.

Esto motiva la necesidad de un modelo m√°s flexible que combine m√∫ltiples tipos de caracter√≠sticas y maneje su heterogeneidad.

---

### 2.2 Clasificador combinado propuesto basado en CNN-LSTM

El modelo propuesto mejora la arquitectura CNN-LSTM tradicional al incorporar m√∫ltiples tipos de caracter√≠sticas por modalidad:

- **Audio**:
  - 2D: espectrogramas ‚Üí procesados con CNN + BiLSTM
  - 1D: LLDs resumidos (HSFs) ‚Üí procesados con DNN

- **Letras**:
  - 2D: Word2Vec ‚Üí procesado con CNN + BiLSTM + atenci√≥n
  - 1D: vectores de frecuencia por prueba chi-cuadrado ‚Üí procesados con DNN

La arquitectura se divide en dos ramas:
- **2D + CNN-LSTM**: para caracter√≠sticas con estructura matricial y secuencial.
- **1D + DNN**: para vectores estad√≠sticos o de frecuencia.

Ambas salidas se concatenan para producir una predicci√≥n unimodal (audio o letras). Posteriormente, estas predicciones se integran en una etapa de fusi√≥n multimodal (ver secci√≥n 3).

**Comentario t√©cnico:**
Este dise√±o evita la necesidad de normalizar o reducir dimensionalmente las caracter√≠sticas para fusionarlas desde el inicio. En su lugar, cada tipo de dato se procesa con la arquitectura m√°s adecuada, y la fusi√≥n se realiza a nivel de etiquetas.

**Comentario t√©cnico**:
Aunque los embeddings de palabras son vectores 1D, al representar una canci√≥n completa como una secuencia de embeddings (uno por palabra), se forma una matriz de tama√±o [n_palabras √ó dimensi√≥n_embedding]. Esta estructura 2D permite aplicar convoluciones espaciales, trat√°ndola como una imagen o mapa de caracter√≠sticas.

---

### 2.3 Descripci√≥n espec√≠fica de los clasificadores de audio y letras

#### 2.3.1 Capa de entrada para clasificaci√≥n de audio

El modelo utiliza dos tipos de caracter√≠sticas ac√∫sticas:

- **Espectrogramas (2D)**:  
  Se generan mediante transformada de Fourier de corto tiempo (STFT), representando la energ√≠a de frecuencias a lo largo del tiempo.  
  Para agregar **secuencia temporal**, se concatenan m√∫ltiples espectrogramas de segmentos consecutivos, formando una estructura tipo "video" o mapa temporal que puede ser procesado por LSTM.  
  Esto permite capturar la evoluci√≥n emocional en el tiempo, no solo patrones est√°ticos.

- **LLDs (Low-Level Descriptors, 1D)**:  
  Son caracter√≠sticas extra√≠das directamente de la se√±al de audio en cada frame o ventana temporal.  
  Incluyen:
  - MFCC (coeficientes cepstrales en escala Mel)
  - ZCR (tasa de cruce por cero)
  - Centroides espectrales
  - Rolloff espectral
  - Flujo espectral
  - Chroma features (relaci√≥n con notas musicales)

  Estas caracter√≠sticas se resumen mediante funciones estad√≠sticas (m√°ximo, media, varianza) para formar los **HSFs (High-Level Statistical Features)**, que se procesan con una red DNN.

**Comentario t√©cnico:**
Los LLDs se extraen de la naturaleza unidimensional de la se√±al de audio, frame por frame. Aunque cada descriptor es escalar o vectorial, su secuencia en el tiempo puede formar una serie temporal, pero aqu√≠ se resumen para formar vectores fijos.

---

#### 2.3.2 Capa de entrada para clasificaci√≥n de letras

El modelo utiliza dos tipos de representaciones textuales:

- **Word Embedding (2D)**:  
  Se extraen con Word2Vec, generando vectores de dimensi√≥n fija para cada palabra.  
  Al representar toda la letra como una secuencia de embeddings, se forma una matriz $[n_{palabras} \times dimensi√≥n_{embedding}]$, que se procesa con CNN para extraer patrones sem√°nticos locales.

- **Vector de frecuencia por prueba chi-cuadrado (1D)**:  
  Se utiliza la prueba **CHI** para seleccionar palabras discriminativas entre clases emocionales.  
  A diferencia de TF-IDF, que pondera frecuencia y rareza, CHI mide la **correlaci√≥n estad√≠stica** entre la presencia de una palabra y una clase emocional.  
  Esto permite seleccionar t√©rminos m√°s relevantes para clasificaci√≥n supervisada.

**Comentario t√©cnico:**
La prueba CHI se basa en la dependencia entre variables: si una palabra aparece significativamente m√°s en canciones de una emoci√≥n espec√≠fica, su valor CHI ser√° alto. Esto mejora la discriminaci√≥n frente a TF-IDF, que no considera etiquetas.


#### 2.3.3 Capa CNN

Para procesar las caracter√≠sticas bidimensionales (espectrogramas y matrices de Word2Vec), se utiliza una capa CNN con arquitectura multiescala. Esta capa permite extraer patrones espaciales locales relevantes para la clasificaci√≥n emocional.

- **Estructura de la capa CNN**:
  - 2 capas de convoluci√≥n + 2 capas de max pooling.
  - Primera convoluci√≥n: 64 filtros de tama√±o $2 \times 2$, stride = 1.
  - Segunda convoluci√≥n: filtros de tama√±o $3 \times 3$.
  - Ambas capas aplican activaci√≥n ReLU.
  - Las salidas se conectan en secuencia para formar una representaci√≥n serializada que se env√≠a a la capa LSTM.

- **Proceso de convoluci√≥n**:
  1. Cada filtro convoluciona localmente sobre la entrada (espectrograma o matriz de embeddings).
  2. Se calcula la activaci√≥n lineal:  
     $h_{1F}(i) = W_{F} \cdot X(i:i+F-1) + b$
  3. Se concatenan las activaciones:  
     $h_{1F} = [h_{1F}(1), h_{1F}(2), \dots, h_{1F}(H)]$
  4. Se aplica ReLU:  
     $h_{r1F} = \text{ReLU}(h_{1F})$

- **Proceso de pooling**:
  - Se aplica max pooling para reducir dimensionalidad y conservar los valores m√°s representativos.
  - La operaci√≥n se adapta al tama√±o de la muestra.
  - Resultado:  
    $h_{rP1F} = \max(h_{r1F})$

- **Salida serializada**:
  - Se concatenan los resultados de todos los filtros:  
    $h_1 = \text{Concatenate}(h_{rP1F_1}, h_{rP1F_2}, \dots)$
  - Esta salida se conecta directamente a la capa LSTM para capturar dependencias temporales.

**Comentario t√©cnico:**
La CNN act√∫a como extractor de patrones locales en espectrogramas y matrices de embeddings. Al aplicar m√∫ltiples filtros y pooling, se obtiene una representaci√≥n compacta pero rica. La serializaci√≥n posterior permite que LSTM procese la secuencia como si fuera una serie temporal, incluso si la entrada original no lo era expl√≠citamente.

#### 2.3.4 Capa BiLSTM y mecanismo de atenci√≥n

Una vez que la CNN ha extra√≠do y serializado las caracter√≠sticas 2D, estas se procesan por una capa BiLSTM (LSTM bidireccional) para capturar dependencias temporales en ambas direcciones (pasado y futuro).

- **BiLSTM**:
  - Contiene 128 unidades en cada direcci√≥n.
  - La salida es una secuencia de vectores:  
    $$ [r_{(1)}, r_{(2)}, r_{(3)}, \dots, r_{(N)}] $$
  - Cada vector $r_{(i)}$ representa la activaci√≥n en el paso temporal $i$, considerando contexto anterior y posterior.

- **Atenci√≥n**:
  - Se aplica un mecanismo de atenci√≥n para ponderar la importancia de cada paso temporal.
  - Se calcula un peso $a_i$ para cada vector $r_{(i)}$ usando softmax sobre una funci√≥n de puntuaci√≥n:  
    $$ a_i = \frac{\exp(f(r_{(i)}))}{\sum_j \exp(f(r_{(j)}))} $$
  - La salida final es una combinaci√≥n ponderada de todos los vectores:  
    $$ \text{att}_n = \sum_i a_i \cdot r_{(i)} $$

Este mecanismo permite que el modelo se enfoque en los momentos m√°s relevantes emocionalmente dentro de la secuencia, mejorando la capacidad de clasificaci√≥n.

**Comentario t√©cnico:**
La BiLSTM permite capturar patrones secuenciales en ambas direcciones, lo cual es √∫til en m√∫sica y texto donde el contexto emocional puede depender de lo que viene antes y despu√©s. El mecanismo de atenci√≥n act√∫a como un selector din√°mico de momentos clave, similar a c√≥mo un humano se enfoca en ciertas frases o pasajes musicales.

#### 2.3.5 Capa DNN

La red neuronal profunda (DNN) se utiliza para procesar las caracter√≠sticas unidimensionales (1D) tanto de audio como de letras:

- **Audio**:  
  Se ingresan los HSFs (estad√≠sticos derivados de LLDs), que resumen la se√±al en vectores compactos.

- **Letras**:  
  Se ingresan los vectores de frecuencia obtenidos por la prueba chi-cuadrado, que representan la relevancia de palabras discriminativas.

La DNN est√° compuesta por:
- 3 capas ocultas con:
  - 256 nodos
  - 128 nodos
  - 64 nodos
- Cada capa aplica funciones de activaci√≥n (presumiblemente ReLU) para sintetizar la informaci√≥n.

**Comentario t√©cnico:**  
La DNN act√∫a como un sintetizador de patrones en vectores fijos, sin necesidad de convoluci√≥n ni secuencias. Es ideal para procesar caracter√≠sticas estad√≠sticas o de frecuencia que no tienen estructura espacial ni temporal.

#### 2.3.6 Capa de salida

La capa de salida se encarga de producir la predicci√≥n emocional para cada modalidad (audio o letras), combinando las representaciones extra√≠das por las ramas 2D y 1D.

- **Componentes**:
  - Capa completamente conectada (FC)
  - Activaci√≥n softmax para clasificaci√≥n multiclase

- **Proceso**:
  1. Se concatenan las salidas de:
     - CNN + BiLSTM + atenci√≥n (caracter√≠sticas 2D)
     - DNN (caracter√≠sticas 1D)
  2. Esta representaci√≥n fusionada se pasa por una capa FC.
  3. Se aplica softmax para obtener probabilidades sobre las clases emocionales.

**Comentario t√©cnico:**  
La arquitectura permite que cada tipo de caracter√≠stica contribuya a la decisi√≥n final sin necesidad de normalizaci√≥n previa. La fusi√≥n se realiza a nivel de representaci√≥n interna, antes de la clasificaci√≥n, lo que preserva la riqueza sem√°ntica y ac√∫stica de cada modalidad.


---

## 3. üîó Fusi√≥n multimodal

El art√≠culo destaca que las t√©cnicas tradicionales de fusi√≥n multimodal ‚Äîcomo la fusi√≥n temprana (feature fusion) y la fusi√≥n tard√≠a (decision fusion)‚Äî presentan limitaciones importantes:
- La fusi√≥n de caracter√≠sticas heterog√©neas requiere normalizaci√≥n o reducci√≥n dimensional, lo que puede causar p√©rdida de informaci√≥n emocional.
- La fusi√≥n de decisiones (por ejemplo, promediar probabilidades) ignora la correlaci√≥n entre modalidades y no permite aprendizaje conjunto.

Para superar estas limitaciones, se propone un m√©todo de *stacking ensemble learning* que integra las salidas de clasificadores unimodales (audio y letras) mediante un subclasificador entrenado sobre sus predicciones.

### 3.1 Construcci√≥n del modelo de stacking
La idea central es utilizar las etiquetas predichas por los clasificadores de audio y letras como nuevas caracter√≠sticas de entrada para un subclasificador que aprende a combinarlas de manera √≥ptima.

**Estructura del modelo de stacking:**

- **Clasificadores base**:
  - Clasificador de audio (CNN-LSTM + DNN)
  - Clasificador de letras (CNN-LSTM + DNN)
  - Cada uno produce una predicci√≥n emocional independiente (softmax)

- **Subclasificador**:
  - Recibe como entrada las etiquetas predichas por los clasificadores base.
  - Aprende a combinar estas salidas para producir una predicci√≥n final m√°s robusta.
  - Se entrena sobre un nuevo conjunto de datos generado a partir de las salidas de los clasificadores base.

**Ventajas del enfoque:**
- Evita la fusi√≥n directa de vectores heterog√©neos.
- Preserva la independencia de los modelos unimodales.
- Permite capturar correlaciones entre predicciones de distintas modalidades.
- Mejora la precisi√≥n sin modificar los clasificadores originales.

**Comentario t√©cnico:**  
El stacking act√∫a como una capa de decisi√≥n aprendida, que reemplaza la simple combinaci√≥n lineal de probabilidades. Es especialmente √∫til cuando las modalidades tienen estructuras y escalas distintas, como ocurre con audio y texto en MIR. En lugar de fusionar directamente las caracter√≠sticas heterog√©neas, se combinan las salidas softmax de los clasificadores unimodales en un nuevo conjunto de datos. Este conjunto representa cada muestra como un vector de predicciones, que luego se utiliza para entrenar un subclasificador supervisado. As√≠, el modelo aprende a corregir errores, detectar patrones complementarios entre modalidades y mejorar la precisi√≥n global sin alterar los modelos base. Esta estrategia convierte la fusi√≥n multimodal en un problema de aprendizaje sobre decisiones, manteniendo la especializaci√≥n de cada rama y evitando la p√©rdida de informaci√≥n por normalizaci√≥n o reducci√≥n dimensional.

### 3.2 Entrenamiento del modelo de stacking

Para evitar el sobreajuste en el subclasificador, se utiliza un esquema de **validaci√≥n cruzada de 5 pliegues** durante el entrenamiento. Este procedimiento permite generar predicciones confiables de los clasificadores base sin reutilizar directamente los datos de entrenamiento, lo que garantiza que el subclasificador aprenda sobre salidas no sobreajustadas.

---

#### 3.2.1 Procesamiento del conjunto de datos

- El conjunto original contiene **2000 muestras** etiquetadas con emociones (enojado, feliz, relajado, triste).
- Se divide en:
  - **80% entrenamiento** (1600 muestras)
  - **20% prueba** (400 muestras)

- Sobre el conjunto de entrenamiento (1600 muestras), se aplica **validaci√≥n cruzada de 5 pliegues**:
  - En cada iteraci√≥n, se entrena el clasificador base (audio o letras) sobre 4 pliegues (1280 muestras) y se predice sobre el pliegue restante (320 muestras).
  - Esto se repite 5 veces, generando predicciones para todo el conjunto de entrenamiento sin reutilizar datos.

- Resultado:
  - Se obtiene un nuevo conjunto de entrenamiento para el subclasificador, compuesto por las **predicciones unimodales** (softmax) de cada muestra.
  - El conjunto de prueba se predice directamente con los clasificadores base entrenados sobre todo el conjunto de entrenamiento.

#### 3.2.2 Entrenamiento de clasificadores base

Cada clasificador (audio y letras) se entrena usando validaci√≥n cruzada de 5 pliegues. En cada iteraci√≥n, se generan predicciones sobre el pliegue de validaci√≥n, que luego se usan como entrada para el subclasificador.

#### 3.2.3 Entrenamiento del subclasificador

Se construye un nuevo conjunto de datos a partir de las predicciones unimodales. Este conjunto se usa para entrenar un subclasificador (capa FC + softmax), que aprende a combinar las decisiones y produce la predicci√≥n final multimodal.

---
## 4. Experimentos

### 4.1 Conjunto de datos

El conjunto utilizado en los experimentos proviene del subset de etiquetas de Last.fm dentro del Million Song Dataset. Siguiendo el modelo emocional de Thayer, se extrajeron listas de canciones etiquetadas con cuatro emociones: **angry**, **happy**, **relaxed** y **sad**.

- Se seleccionaron **500 canciones por emoci√≥n**, totalizando **2000 muestras**.
- Los archivos de audio y letras fueron descargados manualmente mediante scripts, en funci√≥n de las listas de etiquetas obtenidas.

**Comentario t√©cnico:**  
Este proceso implica una curaci√≥n manual del dataset, donde las etiquetas emocionales se derivan de tags p√∫blicos en Last.fm. La construcci√≥n cuidadosa del conjunto garantiza que cada clase est√© balanceada y que las muestras reflejen las emociones objetivo de forma expl√≠cita.

### 4.2 Preprocesamiento de audio

Para mejorar la calidad de las caracter√≠sticas extra√≠das del audio, se aplic√≥ un m√©todo de preprocesamiento en cuatro niveles:

- **Segmentaci√≥n fina**:  
  Se dividieron los audios en clips de 15 segundos para amplificar la informaci√≥n emocional relevante.

- **Separaci√≥n de voz humana**:  
  Se extrajeron dos tipos de fragmentos:
  - Clips de voz humana
  - Clips de fondo musical (sin voz)

- **Construcci√≥n de datasets**:  
  Se generaron cuatro variantes:
  - Audio original de 30s
  - Audio original de 15s
  - Fondo musical de 15s
  - Voz humana de 15s

- **Evaluaci√≥n experimental**:  
  Se compar√≥ el rendimiento de clasificaci√≥n usando LLDs + SVM. Los clips de fondo musical de 15s mostraron la mejor precisi√≥n promedio.

**Comentario t√©cnico:**  
La segmentaci√≥n y separaci√≥n permiten aislar componentes ac√∫sticos m√°s estables y representativos. El fondo musical, al estar libre de variaciones vocales, ofrece una se√±al m√°s uniforme para la extracci√≥n de caracter√≠sticas estad√≠sticas como LLDs.

### 4.3 Experimentos con audio

Se evaluaron distintos modelos de clasificaci√≥n sobre las caracter√≠sticas ac√∫sticas extra√≠das del audio:

- **Modelos comparados**:
  - CNN sobre espectrogramas
  - LSTM sobre espectrogramas
  - CNN-LSTM combinado
  - Modelo propuesto (CNN-LSTM + DNN con espectrogramas + LLDs)

- **Resultados**:
  - El modelo propuesto obtuvo la mejor precisi√≥n promedio (**68%**).
  - La combinaci√≥n de espectrogramas y LLDs permiti√≥ mejorar especialmente la clasificaci√≥n de la emoci√≥n ‚Äúrelajado‚Äù, que era la m√°s dif√≠cil para los modelos simples.

**Comentario t√©cnico:**  
El uso conjunto de caracter√≠sticas 2D (espectrogramas) y 1D (LLDs) permite capturar tanto la estructura temporal como las propiedades estad√≠sticas del audio. La arquitectura CNN-LSTM extrae patrones locales y secuenciales, mientras que la DNN sintetiza informaci√≥n global, logrando una representaci√≥n m√°s robusta.


### 4.4 Experimento de clasificaci√≥n de letras

Se evaluaron distintos modelos de clasificaci√≥n sobre las caracter√≠sticas extra√≠das del texto de las letras:

- **Modelos comparados**:
  - CNN sobre Word2vec
  - LSTM sobre Word2vec
  - CNN-LSTM combinado
  - Modelo propuesto (CNN-LSTM + DNN con Word2vec + chi-cuadrado)

- **Resultados**:
  - El modelo propuesto obtuvo la mejor precisi√≥n promedio (**74%**).
  - La combinaci√≥n de embeddings y vectores estad√≠sticos permiti√≥ mejorar la clasificaci√≥n de emociones como ‚Äútriste‚Äù y ‚Äúfeliz‚Äù.

**Comentario t√©cnico:**  
El uso conjunto de representaciones distribucionales (Word2vec) y estad√≠sticas (chi-cuadrado) permite capturar tanto el contexto sem√°ntico como la frecuencia discriminativa de palabras clave. La arquitectura CNN-LSTM extrae patrones secuenciales, mientras que la DNN sintetiza correlaciones globales entre t√©rminos.

### 4.5 Experimento de fusi√≥n multimodal

Se compararon tres enfoques para integrar las modalidades de audio y letras:

- **Fusi√≥n de caracter√≠sticas**:  
  Se concatenaron los vectores de caracter√≠sticas de ambas modalidades, con normalizaci√≥n previa.  
  ‚Üí Precisi√≥n promedio: **72.4%**

- **Fusi√≥n de decisiones**:  
  Se combinaron las probabilidades de salida de los clasificadores base mediante votaci√≥n lineal.  
  ‚Üí Precisi√≥n promedio: **74.8%**

- **Fusi√≥n por stacking (propuesta)**:  
  Se entren√≥ un subclasificador sobre las salidas softmax de los modelos unimodales.  
  ‚Üí Precisi√≥n promedio: **78.2%**

**Comentario t√©cnico:**  
La fusi√≥n por stacking supera a los m√©todos tradicionales al evitar la p√©rdida de informaci√≥n por normalizaci√≥n o reducci√≥n dimensional. Al aprender sobre las decisiones de cada modalidad, el subclasificador puede detectar patrones complementarios y corregir errores, logrando una integraci√≥n m√°s efectiva.

### 4.6 Experimento comparativo

Se compar√≥ el rendimiento del modelo propuesto con otros enfoques publicados en a√±os recientes, tanto unimodales como multimodales:

- **Modelos unimodales**:
  - LLDs + SVM (audio): 57.1%
  - MIDI + RNN (audio): 56.8%
  - TF-IDF (letras): 62.2%
  - Word2vec + LSTM (letras): 69.3%

- **Modelos multimodales**:
  - Random Forest con fusi√≥n de caracter√≠sticas: 73.8%
  - Fusi√≥n de decisiones con LFSM: 75.8%
  - Fusi√≥n por voto a nivel de oraci√≥n: 80.6%
  - **Modelo propuesto (stacking + multifeatures)**: **78.2%**

**Comentario t√©cnico:**  
El modelo propuesto logra una precisi√≥n competitiva frente a m√©todos multimodales m√°s complejos. Su ventaja radica en la integraci√≥n eficiente de caracter√≠sticas heterog√©neas mediante stacking, sin necesidad de normalizaci√≥n cruzada ni estructuras de fusi√≥n manuales.


---
## 5. Conclusiones

El estudio propone un sistema de clasificaci√≥n emocional musical multimodal que combina audio y letras mediante un modelo de red h√≠brida y una estrategia de ensamblado por stacking. Los principales aportes son:

- Un clasificador combinado que integra caracter√≠sticas 2D (espectrogramas, Word2vec) y 1D (LLDs, chi-cuadrado) mediante una arquitectura CNN-LSTM + DNN.
- Un m√©todo de fusi√≥n multimodal basado en stacking, que evita la p√©rdida de informaci√≥n por normalizaci√≥n cruzada y mejora la precisi√≥n frente a m√©todos tradicionales.
- Un esquema de preprocesamiento de audio que optimiza la segmentaci√≥n y separaci√≥n de voz para mejorar la calidad de las caracter√≠sticas.

El modelo alcanz√≥ una precisi√≥n promedio del **78.2%** en clasificaci√≥n emocional multimodal, superando enfoques unimodales y otros m√©todos de fusi√≥n. Se plantea como una soluci√≥n eficaz y escalable para tareas de MIR con datos heterog√©neos.

**Comentario t√©cnico:**  
La arquitectura modular y el enfoque por stacking permiten adaptar el sistema a distintas combinaciones de caracter√≠sticas y modalidades. Esto lo convierte en una base s√≥lida para proyectos que exploren clasificaci√≥n emocional, recomendaci√≥n musical o an√°lisis afectivo en entornos reales.


## 6. Puntos clave para incorporar en el proyecto

### 6.1 Construcci√≥n del dataset

El art√≠culo no proporciona el dataset final, pero describe un flujo replicable:

- **Fuente base**: Million Song Dataset + etiquetas de Last.fm.
- **Etiquetas emocionales**: angry, happy, relaxed, sad.
- **Curaci√≥n**: 500 canciones por emoci√≥n (2000 en total).
- **Audio**: descargado manualmente; se segmenta en clips de 15s.
- **Letras**: extra√≠das por canci√≥n; no se especifica sincronizaci√≥n con el audio.
- **Separaci√≥n de voz/fondo**: no se detalla el m√©todo, pero puede replicarse con herramientas como Spleeter.

**Recursos √∫tiles para replicaci√≥n**:
- [Subset del Million Song Dataset en Hugging Face](https://huggingface.co/datasets/trojblue/million-song-subset)  
- [Repositorio extendido con etiquetas y audio en GitHub](https://github.com/slettner/lastfm-spotify-tags-sim-userdata)  
- [P√°gina de etiquetas de canciones en Last.fm](https://www.last.fm/tag/)  
- [Herramienta de separaci√≥n de fuentes: Spleeter (Deezer)](https://github.com/deezer/spleeter)

**Flujo sugerido para replicaci√≥n**:
1. Extraer listas de canciones etiquetadas desde Last.fm (API o scraping).
2. Cruzar con MSD para obtener metadatos y audio.
3. Descargar letras desde LyricWiki, Genius o Musixmatch.
4. Separar voz/fondo si se desea replicar esa parte.
5. Balancear clases y construir el dataset final.

---

### 6.2 Aspectos t√©cnicos a adaptar del art√≠culo

- **Fusi√≥n por stacking**: entrenar clasificadores unimodales y combinar sus salidas softmax en un subclasificador.
- **Arquitectura h√≠brida**:
  - Audio: CNN-LSTM + DNN sobre espectrogramas y LLDs.
  - Letras: CNN-LSTM + DNN sobre Word2vec y chi-cuadrado.
- **Preprocesamiento de audio**: segmentaci√≥n en 15s, separaci√≥n de voz si se desea.
- **Embeddings modernos**: reemplazar Word2vec por BERT, DistilBERT o similares.
- **Reducci√≥n de combinaciones**: limitar a 2 variantes de audio para simplificar el pipeline.

**Comentario t√©cnico:**  
Este enfoque permite mantener la esencia multimodal del art√≠culo, adaptando su complejidad a los recursos disponibles. La modularidad del sistema facilita la experimentaci√≥n con distintas arquitecturas, embeddings y estrategias de fusi√≥n, sin comprometer la validez metodol√≥gica.
